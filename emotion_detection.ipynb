{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "latest-nature"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "latest-nature"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMkTYXymXBv3",
        "outputId": "71dbb172-d51d-42cb-b2ad-45448ba61209"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers\n"
      ],
      "id": "bMkTYXymXBv3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "together-bread"
      },
      "source": [
        "# Data Preparation"
      ],
      "id": "together-bread"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "judicial-cedar"
      },
      "source": [
        "## Quick EDA"
      ],
      "id": "judicial-cedar"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "direct-prediction"
      },
      "source": [
        "## Unique emotions"
      ],
      "id": "direct-prediction"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "theoretical-blanket"
      },
      "source": [
        "**Imbalance can be seen in the labels**"
      ],
      "id": "theoretical-blanket"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "driven-prague"
      },
      "source": [
        "### Statistics about length of text"
      ],
      "id": "driven-prague"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rental-hamilton"
      },
      "source": [
        "## Column Encoding"
      ],
      "id": "rental-hamilton"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "marine-eligibility"
      },
      "source": [
        "## Emotion Dataset"
      ],
      "id": "marine-eligibility"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "manual-cologne"
      },
      "outputs": [],
      "source": [
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.df = df.reset_index(drop=True)  # Reset the index to ensure sequential values\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, index):\n",
        "        essay = str(self.df.loc[index, 'essay'])\n",
        "        emotion = self.df.loc[index, 'emotion']\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            essay,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'essay': essay,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'emotion': torch.tensor(emotion, dtype=torch.long)\n",
        "        }"
      ],
      "id": "manual-cologne"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seasonal-explorer"
      },
      "source": [
        "## Tokenizer"
      ],
      "id": "seasonal-explorer"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orange-birthday"
      },
      "outputs": [],
      "source": [
        "# create an instance of the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "id": "orange-birthday"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opened-collaboration"
      },
      "source": [
        "# Model"
      ],
      "id": "opened-collaboration"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "residential-relation"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class BertSentimentClassifier(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_classes):\n",
        "        super(BertSentimentClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 128)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)  # Apply dropout once\n",
        "        output = self.fc1(pooled_output)\n",
        "        logits = self.fc2(output)\n",
        "\n",
        "        return logits"
      ],
      "id": "residential-relation"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fifteen-capitol"
      },
      "outputs": [],
      "source": [
        "bert_model_name = \"bert-base-uncased\"\n",
        "#num_classes = train_data[\"emotion\"].nunique()\n",
        "#model = BertSentimentClassifier(bert_model_name, num_classes)"
      ],
      "id": "fifteen-capitol"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "automotive-huntington"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "id": "automotive-huntington"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eleven-hardware"
      },
      "source": [
        "# Training"
      ],
      "id": "eleven-hardware"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "forward-report"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss, total_accuracy = 0, 0\n",
        "    for data in tqdm(data_loader):\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['emotion'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        logits = outputs.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "        total_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # calculate the average loss\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    avg_acc = total_accuracy / len(data_loader)\n",
        "\n",
        "\n",
        "    return avg_loss, avg_acc"
      ],
      "id": "forward-report"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "functioning-settle"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device):\n",
        "    # set the model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # initialize the loss and accuracy variables\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "\n",
        "    # iterate over the data loader\n",
        "    for data in tqdm(data_loader):\n",
        "        # move the inputs to the device\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['emotion'].to(device)\n",
        "\n",
        "        # disable gradient computation\n",
        "        with torch.no_grad():\n",
        "            # get the model's predictions\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "            # get the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # calculate the number of correct predictions\n",
        "            logits = outputs.detach().cpu().numpy()\n",
        "            label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "            total_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    # calculate the average loss and accuracy\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = total_accuracy / len(data_loader)\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "id": "functioning-settle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adverse-story"
      },
      "outputs": [],
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "id": "adverse-story"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "def calculate_metrics(model, dataloader, device):\n",
        "    model.to(device)\n",
        "    predictions, labels = [], []\n",
        "\n",
        "    val_predictions, val_labels = [], []\n",
        "    for data in val_loader:\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_mask'].to(device)\n",
        "        labels = data['emotion'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        logits = outputs.detach().cpu().numpy()\n",
        "        predicted_labels = np.argmax(logits, axis=1)\n",
        "        val_predictions.extend(predicted_labels)\n",
        "        val_labels.extend(labels.to('cpu').numpy())\n",
        "\n",
        "    f1 = f1_score(val_labels, val_predictions, average='weighted')\n",
        "    precision = precision_score(val_labels, val_predictions, average='weighted')\n",
        "    recall = recall_score(val_labels, val_predictions, average='weighted')\n",
        "    return f1, precision, recall"
      ],
      "metadata": {
        "id": "04hsCJDVJi1E"
      },
      "id": "04hsCJDVJi1E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_encoder(data, column_name):\n",
        "    \"\"\"\n",
        "    Encodes categorical data using LabelEncoder.\n",
        "\n",
        "    Args:\n",
        "        data: Pandas DataFrame containing the data to be encoded.\n",
        "        column_name: Name of the column containing categorical data to be encoded.\n",
        "\n",
        "    Returns:\n",
        "        Pandas DataFrame with the encoded categorical data.\n",
        "    \"\"\"\n",
        "\n",
        "    encoder = LabelEncoder()\n",
        "    encoded_data = encoder.fit_transform(data[column_name])\n",
        "    data[column_name] = encoded_data\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "br8vX5E20Y6Z"
      },
      "id": "br8vX5E20Y6Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "expected-sustainability",
        "outputId": "d4766fc1-4ffc-4df9-d8c5-a03b49bda035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-fa80e0594be7>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data.loc[:, 'emotion'] = le.fit_transform(train_data['emotion'])\n",
            "<ipython-input-30-fa80e0594be7>:29: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  train_data.loc[:, 'emotion'] = le.fit_transform(train_data['emotion'])\n",
            "<ipython-input-30-fa80e0594be7>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_data.loc[:, 'emotion'] = le.transform(val_data['emotion'])\n",
            "<ipython-input-30-fa80e0594be7>:30: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  val_data.loc[:, 'emotion'] = le.transform(val_data['emotion'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.68it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2685 | Train Acc: 0.5447 | Val Loss: 0.9404 | Val Acc: 0.6789 | F1 Score: 0.6759 | Precision: 0.7172 | Recall: 0.6789\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.69it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6806 | Train Acc: 0.7706 | Val Loss: 0.9038 | Val Acc: 0.6981 | F1 Score: 0.6946 | Precision: 0.7099 | Recall: 0.6981\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4067 | Train Acc: 0.8639 | Val Loss: 1.0216 | Val Acc: 0.6948 | F1 Score: 0.6920 | Precision: 0.7072 | Recall: 0.6948\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.69it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2275 | Train Acc: 0.9295 | Val Loss: 1.0744 | Val Acc: 0.7141 | F1 Score: 0.7163 | Precision: 0.7274 | Recall: 0.7141\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1295 | Train Acc: 0.9623 | Val Loss: 1.3110 | Val Acc: 0.6769 | F1 Score: 0.6794 | Precision: 0.7046 | Recall: 0.6769\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1025 | Train Acc: 0.9706 | Val Loss: 1.4264 | Val Acc: 0.6908 | F1 Score: 0.6920 | Precision: 0.6975 | Recall: 0.6908\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0771 | Train Acc: 0.9769 | Val Loss: 1.4209 | Val Acc: 0.7068 | F1 Score: 0.7058 | Precision: 0.7108 | Recall: 0.7068\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.69it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0610 | Train Acc: 0.9822 | Val Loss: 1.5509 | Val Acc: 0.6888 | F1 Score: 0.6897 | Precision: 0.7010 | Recall: 0.6888\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0614 | Train Acc: 0.9807 | Val Loss: 1.6026 | Val Acc: 0.6955 | F1 Score: 0.6949 | Precision: 0.6985 | Recall: 0.6955\n",
            "Epoch 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0647 | Train Acc: 0.9804 | Val Loss: 1.5707 | Val Acc: 0.7008 | F1 Score: 0.7005 | Precision: 0.7048 | Recall: 0.7008\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-fa80e0594be7>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data.loc[:, 'emotion'] = le.fit_transform(train_data['emotion'])\n",
            "<ipython-input-30-fa80e0594be7>:29: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  train_data.loc[:, 'emotion'] = le.fit_transform(train_data['emotion'])\n",
            "<ipython-input-30-fa80e0594be7>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_data.loc[:, 'emotion'] = le.transform(val_data['emotion'])\n",
            "<ipython-input-30-fa80e0594be7>:30: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  val_data.loc[:, 'emotion'] = le.transform(val_data['emotion'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2507 | Train Acc: 0.5559 | Val Loss: 0.9266 | Val Acc: 0.6791 | F1 Score: 0.6770 | Precision: 0.6840 | Recall: 0.6793\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6645 | Train Acc: 0.7780 | Val Loss: 0.9222 | Val Acc: 0.6832 | F1 Score: 0.6830 | Precision: 0.6871 | Recall: 0.6833\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.69it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3942 | Train Acc: 0.8727 | Val Loss: 1.0638 | Val Acc: 0.6831 | F1 Score: 0.6798 | Precision: 0.6880 | Recall: 0.6833\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.69it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2261 | Train Acc: 0.9280 | Val Loss: 1.1946 | Val Acc: 0.6898 | F1 Score: 0.6903 | Precision: 0.6996 | Recall: 0.6900\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.69it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1323 | Train Acc: 0.9613 | Val Loss: 1.2532 | Val Acc: 0.6899 | F1 Score: 0.6905 | Precision: 0.6996 | Recall: 0.6900\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.69it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0767 | Train Acc: 0.9799 | Val Loss: 1.3592 | Val Acc: 0.6903 | F1 Score: 0.6915 | Precision: 0.6978 | Recall: 0.6906\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0797 | Train Acc: 0.9738 | Val Loss: 1.5032 | Val Acc: 0.6846 | F1 Score: 0.6808 | Precision: 0.6846 | Recall: 0.6846\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.69it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0561 | Train Acc: 0.9820 | Val Loss: 1.6544 | Val Acc: 0.6918 | F1 Score: 0.6922 | Precision: 0.7001 | Recall: 0.6919\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0533 | Train Acc: 0.9838 | Val Loss: 1.5751 | Val Acc: 0.6786 | F1 Score: 0.6818 | Precision: 0.6901 | Recall: 0.6786\n",
            "Epoch 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.69it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0533 | Train Acc: 0.9849 | Val Loss: 1.7062 | Val Acc: 0.6900 | F1 Score: 0.6885 | Precision: 0.6943 | Recall: 0.6900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-fa80e0594be7>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data.loc[:, 'emotion'] = le.fit_transform(train_data['emotion'])\n",
            "<ipython-input-30-fa80e0594be7>:29: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  train_data.loc[:, 'emotion'] = le.fit_transform(train_data['emotion'])\n",
            "<ipython-input-30-fa80e0594be7>:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  val_data.loc[:, 'emotion'] = le.transform(val_data['emotion'])\n",
            "<ipython-input-30-fa80e0594be7>:30: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  val_data.loc[:, 'emotion'] = le.transform(val_data['emotion'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2908 | Train Acc: 0.5335 | Val Loss: 0.9125 | Val Acc: 0.6888 | F1 Score: 0.6884 | Precision: 0.6955 | Recall: 0.6886\n",
            "Epoch 2:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.7095 | Train Acc: 0.7615 | Val Loss: 0.8797 | Val Acc: 0.7127 | F1 Score: 0.7103 | Precision: 0.7197 | Recall: 0.7126\n",
            "Epoch 3:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4367 | Train Acc: 0.8560 | Val Loss: 0.9494 | Val Acc: 0.7048 | F1 Score: 0.7093 | Precision: 0.7241 | Recall: 0.7046\n",
            "Epoch 4:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2606 | Train Acc: 0.9212 | Val Loss: 1.1134 | Val Acc: 0.6894 | F1 Score: 0.6870 | Precision: 0.6921 | Recall: 0.6893\n",
            "Epoch 5:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1494 | Train Acc: 0.9593 | Val Loss: 1.2636 | Val Acc: 0.7007 | F1 Score: 0.7008 | Precision: 0.7115 | Recall: 0.7006\n",
            "Epoch 6:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1058 | Train Acc: 0.9714 | Val Loss: 1.4151 | Val Acc: 0.6914 | F1 Score: 0.6949 | Precision: 0.7291 | Recall: 0.6913\n",
            "Epoch 7:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0907 | Train Acc: 0.9731 | Val Loss: 1.4960 | Val Acc: 0.6881 | F1 Score: 0.6907 | Precision: 0.7103 | Recall: 0.6880\n",
            "Epoch 8:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0717 | Train Acc: 0.9784 | Val Loss: 1.4944 | Val Acc: 0.6953 | F1 Score: 0.6918 | Precision: 0.7020 | Recall: 0.6953\n",
            "Epoch 9:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0553 | Train Acc: 0.9835 | Val Loss: 1.5417 | Val Acc: 0.6934 | F1 Score: 0.6929 | Precision: 0.6980 | Recall: 0.6933\n",
            "Epoch 10:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 752/752 [02:40<00:00,  4.70it/s]\n",
            "100%|██████████| 188/188 [00:13<00:00, 13.55it/s]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and split data\n",
        "file_path = 'isear.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df = df.drop(columns=['Unnamed: 2'])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Perform K-fold cross-validation with 80:20 split\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "prec_scores_per_epoch = []\n",
        "rec_scores_per_epoch = []\n",
        "f1_scores_per_epoch = []  # List to store F1 scores for each epoch\n",
        "\n",
        "for train_index, val_index in kfold.split(df):\n",
        "    train_data = df.iloc[train_index]\n",
        "    val_data = df.iloc[val_index]\n",
        "\n",
        "    # Prepare data for BERT model\n",
        "    le = LabelEncoder()\n",
        "    train_data.loc[:, 'emotion'] = le.fit_transform(train_data['emotion'])\n",
        "    val_data.loc[:, 'emotion'] = le.transform(val_data['emotion'])\n",
        "# set the batch size\n",
        "    batch_size = 8\n",
        "\n",
        "# set the maximum sequence length\n",
        "    max_len = 150\n",
        "    train_dataset = EmotionDataset(train_data, tokenizer, max_len)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_dataset = EmotionDataset(val_data, tokenizer, max_len)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize and train the model\n",
        "    num_classes = train_data[\"emotion\"].nunique()\n",
        "    model = BertSentimentClassifier(bert_model_name, num_classes)\n",
        "    lr = 2e-5\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize epoch-wise F1, precision, recall, and loss lists\n",
        "    epoch_prec_scores = []\n",
        "    epoch_rec_scores = []\n",
        "    epoch_f1_scores = []\n",
        "    epoch_val_losses = []\n",
        "    epoch_val_acc  =[]\n",
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}:\")\n",
        "\n",
        "        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        # Calculate F1, precision, and recall scores for validation data\n",
        "        f1, precision, recall = calculate_metrics(model, val_loader, device)\n",
        "\n",
        "        # Append epoch-wise results to lists\n",
        "        epoch_prec_scores.append(precision)\n",
        "        epoch_rec_scores.append(recall)\n",
        "        epoch_f1_scores.append(f1)\n",
        "        epoch_val_losses.append(val_loss)\n",
        "        epoch_val_acc.append(val_acc)\n",
        "\n",
        "        print(f\"Training Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | F1 Score: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "    # Append epoch-wise results to lists of all results\n",
        "    prec_scores_per_epoch.append(epoch_prec_scores)\n",
        "    rec_scores_per_epoch.append(epoch_rec_scores)\n",
        "    f1_scores_per_epoch.append(epoch_f1_scores)\n",
        "    val_losses.append(epoch_val_losses)\n",
        "    val_accuracies.append(epoch_val_acc)\n",
        "\n",
        "# Calculate and print average metrics for each epoch\n",
        "average_prec_scores = np.mean(prec_scores_per_epoch, axis=0)\n",
        "average_rec_scores = np.mean(rec_scores_per_epoch, axis=0)\n",
        "average_f1_scores = np.mean(f1_scores_per_epoch, axis=0)\n",
        "average_val_losses = np.mean(val_losses, axis=0)\n",
        "average_val_acc = np.mean(val_accuracies, axis=0)"
      ],
      "id": "expected-sustainability"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nAverage Precision, Recall, F1 Score, and Validation Loss per Epoch:\")\n",
        "for i, (prec, rec, f1, val_loss,val_acc) in enumerate(zip(average_prec_scores, average_rec_scores, average_f1_scores, average_val_losses,average_val_acc)):\n",
        "    print(f\"Epoch {i + 1}:\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall: {rec:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "    print(f\"Validation Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br3OPUmGzt7k",
        "outputId": "504f8042-0c50-4d8d-d923-7ea4c1d7454e"
      },
      "id": "Br3OPUmGzt7k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Precision, Recall, F1 Score, and Validation Loss per Epoch:\n",
            "Epoch 1:\n",
            "Precision: 0.6883\n",
            "Recall: 0.6869\n",
            "F1 Score: 0.6824\n",
            "Validation Loss: 0.9099\n",
            "Validation Acc: 0.6869\n",
            "Epoch 2:\n",
            "Precision: 0.7226\n",
            "Recall: 0.7111\n",
            "F1 Score: 0.7125\n",
            "Validation Loss: 0.8542\n",
            "Validation Acc: 0.7111\n",
            "Epoch 3:\n",
            "Precision: 0.7155\n",
            "Recall: 0.7002\n",
            "F1 Score: 0.7015\n",
            "Validation Loss: 0.9725\n",
            "Validation Acc: 0.7001\n",
            "Epoch 4:\n",
            "Precision: 0.7157\n",
            "Recall: 0.7104\n",
            "F1 Score: 0.7104\n",
            "Validation Loss: 1.0610\n",
            "Validation Acc: 0.7103\n",
            "Epoch 5:\n",
            "Precision: 0.7088\n",
            "Recall: 0.6936\n",
            "F1 Score: 0.6961\n",
            "Validation Loss: 1.2578\n",
            "Validation Acc: 0.6936\n",
            "Epoch 6:\n",
            "Precision: 0.7090\n",
            "Recall: 0.6965\n",
            "F1 Score: 0.6978\n",
            "Validation Loss: 1.3372\n",
            "Validation Acc: 0.6964\n",
            "Epoch 7:\n",
            "Precision: 0.7057\n",
            "Recall: 0.6945\n",
            "F1 Score: 0.6945\n",
            "Validation Loss: 1.4280\n",
            "Validation Acc: 0.6945\n",
            "Epoch 8:\n",
            "Precision: 0.7078\n",
            "Recall: 0.6941\n",
            "F1 Score: 0.6956\n",
            "Validation Loss: 1.4890\n",
            "Validation Acc: 0.6941\n",
            "Epoch 9:\n",
            "Precision: 0.7069\n",
            "Recall: 0.6994\n",
            "F1 Score: 0.7007\n",
            "Validation Loss: 1.4721\n",
            "Validation Acc: 0.6994\n",
            "Epoch 10:\n",
            "Precision: 0.6999\n",
            "Recall: 0.6879\n",
            "F1 Score: 0.6882\n",
            "Validation Loss: 1.6016\n",
            "Validation Acc: 0.6878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = df['essay'][5874]\n",
        "sample_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wkte1vIu_Yxq",
        "outputId": "e5a00852-94fa-42d5-bb2b-4cbe5a546b0d"
      },
      "id": "wkte1vIu_Yxq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'see toilet unclean'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "planned-medicine",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f17cbf31-7611-4077-f5ee-4f2fa6ef79aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Teks: see toilet unclean\n",
            "   Tokens: ['see', 'toilet', 'uncle', '##an']\n",
            "Token IDs: [2156, 11848, 4470, 2319]\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.tokenize(sample_text)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(f' Teks: {sample_text}')\n",
        "print(f'   Tokens: {tokens}')\n",
        "print(f'Token IDs: {token_ids}')"
      ],
      "id": "planned-medicine"
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  sample_text,\n",
        "  max_length=14,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  #pad_to_max_length=True,\n",
        "  padding='max_length',\n",
        "  #truncation=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eacJBhR05eGk",
        "outputId": "4fdf8f38-e612-47b2-9a06-754a43c92c94"
      },
      "id": "eacJBhR05eGk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "input_ids = encoding['input_ids'].to(device)\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqq0_orE5gr1",
        "outputId": "89b8623f-1db0-4144-ef14-011811119bf7"
      },
      "id": "kqq0_orE5gr1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  2156, 11848,  4470,  2319,   102,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "attention_mask = encoding['attention_mask'].to(device)\n",
        "attention_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTqfF1aw5nAe",
        "outputId": "1617d25e-cff4-4e1a-bada-64cebeddcb1a"
      },
      "id": "DTqfF1aw5nAe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_KAEO9x5ryh",
        "outputId": "13e2163f-8106-4ac2-cb34-69960dd7e24b"
      },
      "id": "E_KAEO9x5ryh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'see',\n",
              " 'toilet',\n",
              " 'uncle',\n",
              " '##an',\n",
              " '[SEP]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]',\n",
              " '[PAD]']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(\n",
        "          input_ids = input_ids,\n",
        "          attention_mask = attention_mask\n",
        "      )"
      ],
      "metadata": {
        "id": "4L-C0uvH5t75"
      },
      "id": "4L-C0uvH5t75",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHi92FJn5wOk",
        "outputId": "4739d5cd-bd28-41e3-85e6-2d75159eea34"
      },
      "id": "HHi92FJn5wOk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-3.1357e-03,  7.3692e+00, -7.8224e-01, -2.4882e+00, -1.5418e+00,\n",
              "         -6.3470e-01, -5.1442e-01]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, preds = torch.max(outputs, dim=1)"
      ],
      "metadata": {
        "id": "UbKO_sDq5yrq"
      },
      "id": "UbKO_sDq5yrq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qxXiU6j50jY",
        "outputId": "0a5fe85a-d895-4983-8921-b69449f12518"
      },
      "id": "4qxXiU6j50jY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "probs = F.softmax(outputs, dim=1)\n",
        "probs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChiENekN54Ir",
        "outputId": "23019608-d458-4572-bd65-e30f2d39188c"
      },
      "id": "ChiENekN54Ir",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.2724e-04, 9.9819e-01, 2.8779e-04, 5.2263e-05, 1.3465e-04, 3.3354e-04,\n",
              "         3.7617e-04]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hQD4uWK055GW"
      },
      "id": "hQD4uWK055GW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2553.750365,
      "end_time": "2023-04-25T16:26:50.808250",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-04-25T15:44:17.057885",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}